# -*- coding: utf-8 -*-
"""Llava-Benchmark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eTm-m5Z-D-By93Hj-TCA21Vm64G-PqhI

# Running Llava: a large multi-modal model on Google Colab

Run Llava model on a Google Colab!

Llava is a multi-modal image-text to text model that can be seen as an "open source version of GPT4". It yields to very nice results as we will see in this Google Colab demo.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/62441d1d9fdefb55a0b7d12c/FPshq08TKYD0e-qwPLDVO.png)

The architecutre is a pure decoder-based text model that takes concatenated vision hidden states with text hidden states.

We will leverage QLoRA quantization method and use `pipeline` to run our model.
"""

!pip install -q -U transformers==4.37.2
!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0

"""## Load an image

Let's use the image that has been used for Llava demo

And ask the model to describe that image!
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd '/content/drive/My Drive'
import requests
from PIL import Image

"""## Preparing the quantization config to load the model in 4bit precision

In order to load the model in 4-bit precision, we need to pass a `quantization_config` to our model. Let's do that in the cells below
"""

import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

"""## Load the model using `pipeline`

We will leverage the `image-to-text` pipeline from transformers !
"""

from transformers import pipeline

model_id = "llava-hf/llava-1.5-7b-hf"

pipe = pipeline("image-to-text", model=model_id, model_kwargs={"quantization_config": quantization_config})

"""It is important to prompt the model wth a specific format, which is:
```bash
USER: <image>\n<prompt>\nASSISTANT:
```
"""

from pathlib import Path
import re
from PIL import Image
import os
import csv

images_path = Path("/content/drive/MyDrive/BenchMark")

images = [p for p in images_path.glob("**/*.jpg")]

questions = [
    '0', # 5_o_Clock_Shadow
    'Does the person have Arched Eyebrows?',
    'Is the person Attractive?',
    'Are there Bags Under Eyes?',
    'Is the person bald?',
    'Are there hair bangs?',
    'Does the person have Big Lips?',
    'Does the person have Big Nose?' ,
    'What is the hair color?', # black, blonde, blurry, brown
    'Are there Bushy Eyebrows?',
    'Is the person Chubby?',
    'Does the person have Double Chin?',
    'Is the person wearing glasses?',
    'Is there a Goatee?',
    '0', # Gray_Hair
    'Is the person wearing Heavy Makeup?',
    'Does the person have High Cheekbones?',
    'What is the gender?', # male
    'Is the Mouth Slightly Open?',
    'Does the person have Mustache?',
    'Are the eyes narrow?',
    'Does the person have Beard?',
    'Is the face shape Oval?',
    '0', # pale skin
    '0', # pointy nose
    '0', # receding hairline
    '0', # rosy cheeks
    '0', # sideburns
    'What is the emotional expression of the person in the photo?', # smiling
    '0', # straight hair
    '0', # wavy hair
    'What Accessories are present in the photo?', # Wearing_Earrings, Wearing_Hat, Wearing_Lipstick, Wearing_Necklace, Wearing_Necktie
    'What is the age?', # young
    ]

def analyze_image(image):
    answers = []

    for question in questions:

        if question == '0':
            answers.append(0)
            continue

        prompt = f"USER: <image>\n{question}\nASSISTANT:"
        outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})

        text = outputs[0]["generated_text"]
        match = re.search(r"ASSISTANT:\s+(\w+)", text)
        word = match.group(1) if match else "Not found"

        match2 = re.search(r"ASSISTANT:\s+(.+)", text, re.DOTALL)
        text2 = match2.group(1).strip() if match2 else "Not found"

        if question == "Is the person Attractive?":
            if "attractive" in text2.lower() or word.lower().startswith('yes'):
                answers.append(1)
            else:
                answers.append(-1)
            continue

        if question == "What is the gender?":
            if "male" in text2.lower():
                answers.append(1)
            else:
                answers.append(-1)
            continue

        if question == "What is the hair color?":
            hair_colors = ['black', 'blonde', 'blurry', 'brown']
            for color in hair_colors:
                if color == 'blurry':
                    answers.append(0)
                elif color in text2.lower():
                    answers.append(1)
                else:
                    answers.append(-1)
            continue

        if question == "What is the emotional expression of the person in the photo?":
            if "happy" or "smiling" or "happiness" in text2.lower():
                answers.append(1)
            else:
                answers.append(-1)
            continue

        if question == "What Accessories are present in the photo?":
            accessories = ['earrings', 'hat', 'lipstick', 'necklace', 'necktie']
            for accessory in accessories:
                if accessory in text2.lower():
                    answers.append(1)
                else:
                    answers.append(-1)
            continue

        if question == "What is the age?":
            if "young" or "youth" or "teen" in text2.lower():
                answers.append(1)
            else:
                answers.append(-1)
            continue

        if word.lower().startswith('yes'):
            answers.append(1)

        elif word.lower().startswith('no'):
            answers.append(-1)

        else:
            answers.append(text2)


    return answers

images_analysis = {}

for img_path in images:
  image_file_name = img_path.name
  image = Image.open(img_path)
  answers = analyze_image(image)
  images_analysis[image_file_name] = answers

  # print(f"Results for {image_file_name}:\n")
  # for answer in answers:
  #     print(f"{answer}")
  # print("\n------------\n")

csv_file_name = 'result.csv'

with open(csv_file_name, mode='w', newline='') as file:
    writer = csv.writer(file, delimiter=' ')
    for image_file_name, answers in images_analysis.items():
        row = [image_file_name] + answers
        writer.writerow(row)

print(f"Analysis results written to {csv_file_name}.")

